{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.getcwd()+\"/idebate.json\"\n",
    "Glove_path=os.getcwd()+\"/glove.6B.50d.txt\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        raw_data=file.read()\n",
    "        json_data=json.loads(raw_data)\n",
    "    dataframe=pd.DataFrame(columns=[\"text\",\"summary\"])\n",
    "    for i in json_data:\n",
    "        text_summary_pair={}\n",
    "        text=\" \"\n",
    "        for x in i[\"_argument_sentences\"]:\n",
    "            text=text+i[\"_argument_sentences\"][x].lower()\n",
    "            #probably we can get rid of stopping word and punctuaton for input\n",
    "        text_summary_pair[\"text\"]=text\n",
    "        text_summary_pair[\"summary\"]=i[\"_claim\"]\n",
    "        dataframe=dataframe.append(text_summary_pair,ignore_index=True)#append doesn't happen in place\n",
    "    return dataframe\n",
    "##########################################\n",
    "dataframe=load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "#60% training, 20%testing 20%evaluation\n",
    "def data_split(dataframe):\n",
    "    train_dataframe,testandvalidation_dataframe=train_test_split(dataframe,test_size=0.4)\n",
    "    test_dataframe,evaluation_dataframe=train_test_split(testandvalidation_dataframe,test_size=0.5)\n",
    "    return train_dataframe,test_dataframe, evaluation_dataframe\n",
    "########################################## \n",
    "train_dataframe,test_dataframe, evaluation_dataframe=data_split(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1999 .trade , income disparity and poverty .d...</td>\n",
       "      <td>Free trade is good for development and growth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>such a ban would be limited such as being onl...</td>\n",
       "      <td>There is precedent for putting restrictions on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>so for example if mississippi and new mexico ...</td>\n",
       "      <td>Being a federal state helps large states deal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>wwf , african rhino poaching crisis zapwing ,...</td>\n",
       "      <td>Poaching is becoming more advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>a major problem with this policy is that it p...</td>\n",
       "      <td>Gives power to military coup leaders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "29     1999 .trade , income disparity and poverty .d...   \n",
       "1213   such a ban would be limited such as being onl...   \n",
       "1926   so for example if mississippi and new mexico ...   \n",
       "736    wwf , african rhino poaching crisis zapwing ,...   \n",
       "1342   a major problem with this policy is that it p...   \n",
       "\n",
       "                                                summary  \n",
       "29       Free trade is good for development and growth.  \n",
       "1213  There is precedent for putting restrictions on...  \n",
       "1926  Being a federal state helps large states deal ...  \n",
       "736                  Poaching is becoming more advanced  \n",
       "1342               Gives power to military coup leaders  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " there may be the possibility of winning a big prize , but the overwhelming likelihood is that a gambler will lose money .instead , governments should be promoting values like thrift , hard work and self-reliance rather than encouraging or even allowing gambling to promote its own negative values .those in society who most need to self-improve , never do .instead , they tie their hopes and dreams to the lottery .it also sends out the message that success should not necessarily be the result of merit and effort .as a philosophy , gambling culture is incredibly dangerous .gambling makes people concentrate of winning money .religious leaders of all denominations see gambling as eroding family values because it implies that material goods should be valued above other things like friendships and families .\n"
     ]
    }
   ],
   "source": [
    "print(dataframe[\"text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "##################################################\n",
    "word_vec=loadGloveModel(Glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <GO>,<EOS>,<PAD> to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return word_list and vector_list\n",
    "words_list=[]\n",
    "vectors_list=[]\n",
    "for i in word_vec:\n",
    "    words_list.append(i)\n",
    "    vectors_list.append(word_vec[i])\n",
    "    \n",
    "GO_vector=[1]*50\n",
    "EOS_vector=[2]*50\n",
    "PAD_vector=[0]*50\n",
    "words_list.append(\"<GO>\")\n",
    "vectors_list.append(GO_vector)\n",
    "words_list.append(\"<EOS>\")\n",
    "vectors_list.append(EOS_vector)\n",
    "\n",
    "words_list.insert(0,\"<PAD>\")\n",
    "vectors_list.insert(0,PAD_vector)\n",
    "vectors_array=np.array([np.array(xi) for xi in vectors_list])\n",
    "Vectors_array=tf.constant(vectors_array, name=\"vectors_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_len=[]\n",
    "text_len=[]\n",
    "max_text_len=0\n",
    "max_sum_len=0\n",
    "for i in dataframe[\"text\"]:\n",
    "    text_len.append(len(i.split()))\n",
    "    if len(i.split())>max_text_len:\n",
    "        max_text_len=len(i.split())\n",
    "        \n",
    "\n",
    "for i in dataframe[\"summary\"]:\n",
    "    sum_len.append(len(i.split()))\n",
    "    if len(i.split())>max_sum_len:\n",
    "        max_sum_len=len(i.split())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(text_len,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sum_len,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtextlength=400\n",
    "maxsumlength=25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataframe finished\n",
      "train_dataframe finished\n",
      "evaluation_dataframe finished\n"
     ]
    }
   ],
   "source": [
    "def text2ids(dataframe):\n",
    "    numfiles=dataframe.shape[0]\n",
    "    text_ids=[]\n",
    "    #np.zeros((numfiles, maxtextlength), dtype='int32')\n",
    "    for text in dataframe[\"text\"]:\n",
    "        sentence_ids=[]\n",
    "        text_word_list=text.split()\n",
    "        for word in text_word_list:\n",
    "            try:\n",
    "                word_index=words_list.index(word)\n",
    "            except ValueError:\n",
    "                word_index=words_list.index(\"unk\")\n",
    "            sentence_ids.append(word_index)\n",
    "        text_ids.append(sentence_ids)\n",
    "    return text_ids\n",
    "\n",
    "test_text_ids=text2ids(test_dataframe)\n",
    "print(\"test_dataframe finished\" )\n",
    "train_text_ids=text2ids(train_dataframe)\n",
    "print(\"train_dataframe finished\" )\n",
    "evaluation_text_ids=text2ids(evaluation_dataframe)\n",
    "print(\"evaluation_dataframe finished\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataframe finished\n",
      "train_dataframe finished\n",
      "evalution_dataframe finished\n"
     ]
    }
   ],
   "source": [
    "def summary2ids(dataframe):\n",
    "    summary_ids = []\n",
    "    for summary in dataframe[\"summary\"]:\n",
    "        summary_word_list=summary.split()\n",
    "        summary_word_list.append(\"<EOS>\")\n",
    "        sentence_ids=[]\n",
    "        for word in summary_word_list:\n",
    "            try:\n",
    "                word_index=words_list.index(word)\n",
    "            except ValueError:\n",
    "                word_index=words_list.index(\"unk\")\n",
    "            sentence_ids.append(word_index)\n",
    "            \n",
    "        summary_ids.append(sentence_ids)\n",
    "    return summary_ids\n",
    "#################################################\n",
    "test_summary_ids=summary2ids(test_dataframe)\n",
    "print(\"test_dataframe finished\")\n",
    "train_summary_ids=summary2ids(train_dataframe)\n",
    "print(\"train_dataframe finished\")\n",
    "evaluation_summary_ids=summary2ids(evaluation_dataframe) \n",
    "print(\"evalution_dataframe finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding_dim\n",
    "#batchsize\n",
    "\n",
    "\n",
    "#text_input_data\n",
    "#enc_rnn_node //how many node for a single layer\n",
    "#num_layers //how many layers are there\n",
    "#text_sequence_length //\n",
    "#encoding_embedding_dim //50 for example\n",
    "def get_encoder_layer(batch_size,\n",
    "                      text_input_data,\n",
    "                      enc_rnn_node, \n",
    "                       num_layers,\n",
    "                      text_sequence_length,\n",
    "                      encoding_dim):\n",
    "    \n",
    "   \n",
    "    #encoder_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "    embedding_data = tf.Variable(tf.zeros([batch_size, maxtextlength, encoding_dim]),dtype=tf.float32)\n",
    "    embedding_data = tf.nn.embedding_lookup(Vectors_Array,text_input_data)  #final input for encoder\n",
    "    embedding_data=tf.cast(embedding_data,tf.float32)\n",
    "\n",
    "    # a single RNN cell\n",
    "    def get_lstm_cell(enc_rnn_node):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(enc_rnn_node, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return lstm_cell\n",
    "    \n",
    "    #cell here stands for a Rnn single layer\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(enc_rnn_node) for _ in range(num_layers)])   #why not use concatenate function??????\n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                      embedding_data,\n",
    "                                                      sequence_length=text_sequence_length, \n",
    "                                                      dtype=tf.float32)\n",
    "    return encoder_output, encoder_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(batch_size,\n",
    "                   sum_input_data,\n",
    "                   dec_rnn_node,\n",
    "                   num_layers,\n",
    "                   sum_sequence_length,\n",
    "                   vocab_size,\n",
    "                   encoding_dim,\n",
    "                   encoder_state,\n",
    "                   go_index,\n",
    "                   eos_index,\n",
    "                   max_sum_sequence_length):\n",
    "\n",
    "    def process_decoder_input(sum_input_data, batch_size,go_index):\n",
    "        \n",
    "        # cut掉最后一个字符\n",
    "        #endding is the input which already get rid of the endding\n",
    "        ending = tf.strided_slice(sum_input_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "        processed_decoder_input = tf.concat([tf.fill([batch_size, 1], go_index), ending], 1)\n",
    "        return processed_decoder_input\n",
    "    \n",
    "    processed_sum_input_data=process_decoder_input(sum_input_data,batch_size,go_index)\n",
    "    \n",
    "    # 1. Embedding\n",
    "    \n",
    "    \n",
    "    #target_vocab_size = len(100000)  #total \n",
    "    #decoder_embeddings = tf.Variable(tf.random_uniform([400003, 50]))\n",
    "    #decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings,processed_sum_input_data )\n",
    "    #Vectors_Array = tf.Variable(tf.constant(0.0, shape=[400003, 50]), trainable=False, name=\"vectors_array\")\n",
    "    \n",
    "    #decoder_embedding_data = tf.Variable(tf.zeros([batch_size, maxsumlength, encoding_dim]),dtype=tf.float32)\n",
    "    decoder_embedding_data = tf.nn.embedding_lookup(Vectors_Array,processed_sum_input_data)  #final input for decoder \n",
    "    decoder_embedding_data=tf.cast(decoder_embedding_data,tf.float32) \n",
    "    \n",
    "    # 2. 构造Decoder中的RNN单元\n",
    "    def get_decoder_cell(dec_rnn_node):\n",
    "        print(\"so far so good?\")\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(dec_rnn_node,\n",
    "                                               initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))  #initialize what????????????\n",
    "        print(\"so far so good!\")\n",
    "        return decoder_cell\n",
    "    \n",
    "    \n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(dec_rnn_node) for _ in range(num_layers)])\n",
    "     \n",
    "\n",
    "    output_layer = Dense(vocab_size,kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    # 4. Training decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        # 得到help对象\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embedding_data,\n",
    "                                                            sequence_length=sum_sequence_length,\n",
    "                                             \n",
    "                                                            time_major=False) #what is time major\n",
    "        # 构造decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                           training_helper,\n",
    "                                                           encoder_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        training_decoder_output,_ ,_= tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,maximum_iterations=max_sum_sequence_length)\n",
    "        print(\"training decoder is built\")                                                       \n",
    "    # 5. Predicting decoder\n",
    "    # 与training共享参数\n",
    "    \n",
    "    #multiply = tf.constant([3])\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile([tf.constant((go_index), dtype=tf.int32)],\n",
    "                               [tf.constant(batch_size)], \n",
    "                               name='start_tokens')\n",
    "        \n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(Vectors_Array,start_tokens,eos_index)\n",
    "        \n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                             predicting_helper,\n",
    "                                                             encoder_state,\n",
    "                                                             output_layer)\n",
    "        print(\"predicting decoder is build\")\n",
    "        \n",
    "        predicting_decoder_output, _,_ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,impute_finished=True,maximum_iterations=max_sum_sequence_length)#,\n",
    "                                                                         #maximum_iterations=max_sum_sequence_length)\n",
    "    return training_decoder_output, predicting_decoder_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(batch_size,\n",
    "                  text_input_data,\n",
    "                  sum_input_data,\n",
    "                  sum_sequence_length,\n",
    "                  max_sum_sequence_length, \n",
    "                  text_sequence_length,\n",
    "                  vocab_size,\n",
    "                  encoding_dim,  \n",
    "                  enc_rnn_node,\n",
    "                  dec_rnn_node,\n",
    "                  num_layers,\n",
    "                  vectors_array,\n",
    "                  go_index,\n",
    "                  eos_index):\n",
    "    # 获取encoder的状态输出\n",
    "    _, encoder_state = get_encoder_layer(batch_size,\n",
    "                                         text_input_data,\n",
    "                                         enc_rnn_node,\n",
    "                                         num_layers,\n",
    "                                         text_sequence_length,\n",
    "                                         encoding_dim)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 将状态向量与输入传递给decoder\n",
    "    training_decoder_output, predicting_decoder_output = decoding_layer(batch_size,\n",
    "                                                                        sum_input_data,\n",
    "                                                                        dec_rnn_node,\n",
    "                                                                        num_layers,\n",
    "                                                                        sum_sequence_length,\n",
    "                                                                        vocab_size,\n",
    "                                                                        encoding_dim,\n",
    "                                                                        encoder_state,\n",
    "                                                                        go_index,\n",
    "                                                                        eos_index,\n",
    "                                                                        max_sum_sequence_length) \n",
    "    \n",
    "    return training_decoder_output, predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "batch_size = 100\n",
    "enc_rnn_node = 400\n",
    "dec_rnn_node=400\n",
    "num_layers = 2\n",
    "Learning_Rate = 0.1\n",
    "vocab_size=len(words_list)\n",
    "go_index=words_list.index(\"<GO>\")\n",
    "eos_index=words_list.index(\"<EOS>\")\n",
    "pad_index=words_list.index(\"<PAD>\")\n",
    "encoding_dim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400003, 50)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "so far so good?\n",
      "so far so good!\n",
      "so far so good?\n",
      "so far so good!\n",
      "training decoder is built\n",
      "predicting decoder is build\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    Vectors_Array = tf.Variable(tf.constant(0.0, shape=[400003, 50]), trainable=False, name=\"vectors_array\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [400003, 50])\n",
    "    embedding_init = Vectors_Array.assign(embedding_placeholder)\n",
    "    text_input_data = tf.placeholder(tf.int32, [None, None], name='text_input_data')\n",
    "    sum_input_data = tf.placeholder(tf.int32, [None, None], name='sum_input_data')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    sum_sequence_length = tf.placeholder(tf.int32, (None,), name='sum_sequence_length')\n",
    "    max_sum_sequence_length = tf.reduce_max(sum_sequence_length, name='max_sum_len')\n",
    "    text_sequence_length = tf.placeholder(tf.int32, (None,), name='text_sequence_length')\n",
    "    print(\"A\")\n",
    "    \n",
    "    training_decoder_output, predicting_decoder_output = seq2seq_model(batch_size,\n",
    "                                                                       text_input_data,\n",
    "                                                                       sum_input_data,\n",
    "                                                                       sum_sequence_length,\n",
    "                                                                       max_sum_sequence_length,\n",
    "                                                                       text_sequence_length,\n",
    "                                                                       vocab_size,\n",
    "                                                                       encoding_dim,\n",
    "                                                                       enc_rnn_node,\n",
    "                                                                       dec_rnn_node,\n",
    "                                                                       num_layers,\n",
    "                                                                       Vectors_Array,\n",
    "                                                                       go_index,\n",
    "                                                                       eos_index) \n",
    "    print(\"B\")\n",
    "    \n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, name='logits')   \n",
    "    print(\"C\")\n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id, name='predictions')\n",
    "    print(\"D\")\n",
    "    masks = tf.sequence_mask(sum_sequence_length, max_sum_sequence_length, dtype=tf.float32, name='masks')\n",
    "    \n",
    "    print(\"E\")\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        #crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs, logits=logits)\n",
    "        #train_loss = (tf.reduce_sum(crossent * target_weights) /batch_size)\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits,sum_input_data,masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sum_input_data:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(Inputs,batch_size,pad_index):\n",
    "    padded_inputs=[]\n",
    "    length_list=[]\n",
    "    for Input in Inputs:\n",
    "        if len(Input)>=400:\n",
    "            padded_input=Input[:400]\n",
    "            length=len(padded_input)\n",
    "        else:\n",
    "            length=len(Input)\n",
    "            #padded_input=Input.extend([0]*(400-length))\n",
    "            padded_input=np.pad(Input,(0,400-length),\"constant\",constant_values=0)\n",
    "        length_list.append(length)\n",
    "        padded_input=np.asarray(padded_input)\n",
    "        padded_inputs.append(padded_input)\n",
    "    padded_inputs=np.asarray(padded_inputs) \n",
    "    return padded_inputs,length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_batches(text_ids,summary_ids, batch_size,pad_index):\n",
    "    batch_i=randint(0, len(text_ids)//batch_size)\n",
    "    start_i = batch_i * batch_size\n",
    "    text_batch = text_ids[start_i:start_i + batch_size]\n",
    "    summary_batch = summary_ids[start_i:start_i + batch_size]\n",
    "    pad_text_batch,text_sequence_length = padding(text_batch,batch_size,pad_index)\n",
    "    pad_summary_batch,sum_sequence_length = padding(summary_batch,batch_size,pad_index)\n",
    "    return pad_text_batch, pad_summary_batch, text_sequence_length,sum_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(text_ids,summary_ids, batch_size,pad_index):\n",
    "    for batch_i in range(0, len(text_ids)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        text_batch = text_ids[start_i:start_i + batch_size]\n",
    "        summary_batch = summary_ids[start_i:start_i + batch_size]\n",
    "        pad_text_batch,text_sequence_length = padding(text_batch,batch_size,pad_index)\n",
    "        pad_summary_batch,sum_sequence_length = padding(summary_batch,batch_size,pad_index)\n",
    "        yield pad_text_batch, pad_summary_batch, text_sequence_length,sum_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n\nCaused by op 'optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-106-dfc80fd16352>\", line 43, in <module>\n    cost = tf.contrib.seq2seq.sequence_loss(training_logits,sum_input_data,masks)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\", line 92, in sequence_loss\n    labels=targets, logits=logits_flat)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2042, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4753, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-0bc3e1dab79d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLearning_Rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                  \u001b[0mtext_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mText_Sequence_Length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                  sum_sequence_length:Sum_Sequence_Length})\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n\nCaused by op 'optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-106-dfc80fd16352>\", line 43, in <module>\n    cost = tf.contrib.seq2seq.sequence_loss(training_logits,sum_input_data,masks)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\", line 92, in sequence_loss\n    labels=targets, logits=logits_flat)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2042, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4753, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "display_step = 2\n",
    "checkpoint = \"trained_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: vectors_array})\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (train_text_batch, train_summary_batch, Text_Sequence_Length,Sum_Sequence_Length) in enumerate(\n",
    "            get_batches(train_text_ids, train_summary_ids, batch_size,pad_index)):\n",
    "            \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {text_input_data:train_text_batch,\n",
    "                 sum_input_data: train_summary_batch,\n",
    "                 learning_rate: Learning_Rate,\n",
    "                 text_sequence_length:Text_Sequence_Length,\n",
    "                 sum_sequence_length:Sum_Sequence_Length})\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                valid_text_batch,valid_summary_batch,valid_text_length,valid_summary_length=get_valid_batches()\n",
    "                \n",
    "                # 计算validation loss\n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {text_input_data: valid_text_batch,\n",
    "                 summary_input_data: valid_summary_batch,\n",
    "                 learning_rate: Learning_Rate,\n",
    "                 text_sequence_length: valid_text_length,\n",
    "                 sum_sequence_length: valid_summary_length})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Training Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_text_ids) // batch_size, \n",
    "                              loss, \n",
    "                              validation_loss[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_summary_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Text_Sequence_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_batches(text_ids,summary_ids, batch_size,pad_index):\n",
    "    batch_i=randint(0, len(text_ids)//batch_size)\n",
    "    start_i = batch_i * batch_size\n",
    "    text_batch = text_ids[start_i:start_i + batch_size]\n",
    "    summary_batch = summary_ids[start_i:start_i + batch_size]\n",
    "    pad_text_batch,text_sequence_length = padding(text_batch,batch_size,pad_index)\n",
    "    pad_summary_batch,sum_sequence_length = padding(summary_batch,batch_size,pad_index)\n",
    "    return pad_text_batch, pad_summary_batch, text_sequence_length,sum_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb, pmb, tsl,ssl=get_valid_batches(test_text_ids,test_summary_ids, batch_size,pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decode/decoder/transpose:0' shape=(100, ?, 400003) dtype=float32>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_decoder_output.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_text_batch,text_sequence_length = padding(text_batch,batch_size,pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decode/decoder/transpose:0' shape=(100, ?, 400003) dtype=float32>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_decoder_output.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n\nCaused by op 'optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-55-b5d682de0072>\", line 45, in <module>\n    masks)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\", line 92, in sequence_loss\n    labels=targets, logits=logits_flat)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2042, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4753, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-781629f6609b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtext_input_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_text_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum_input_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_summary_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLearning_Rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mText_Sequence_Length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mSum_Sequence_Length\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n\nCaused by op 'optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-55-b5d682de0072>\", line 45, in <module>\n    masks)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\", line 92, in sequence_loss\n    labels=targets, logits=logits_flat)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2042, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4753, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/yutian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [3900,400003] and labels shape [40000]\n\t [[Node: optimization/sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](optimization/sequence_loss/Reshape, optimization/sequence_loss/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run([train_op, cost],{text_input_data:train_text_batch,sum_input_data: train_summary_batch,learning_rate: Learning_Rate,text_sequence_length:Text_Sequence_Length,sum_sequence_length:Sum_Sequence_Length})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
